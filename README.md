# DocIA - Sistema de Busca Inteligente de Documentos ğŸ§ ğŸ“š

Sistema avanÃ§ado de busca e anÃ¡lise de documentos com IA, oferecendo respostas em linguagem natural baseadas no conteÃºdo dos seus arquivos.

## ğŸš€ CaracterÃ­sticas Principais

- **Busca SemÃ¢ntica Inteligente**: Encontra informaÃ§Ãµes mesmo sem correspondÃªncia exata de palavras
- **Respostas em Linguagem Natural**: IA gera respostas diretas e contextualizadas
- **MÃºltiplos Formatos**: Suporte para PDF, DOCX e TXT
- **Interface Web Moderna**: Interface limpa e responsiva
- **Monitoramento AutomÃ¡tico**: Detecta alteraÃ§Ãµes nos documentos automaticamente
- **IA Local**: Usa Ollama com modelo Mistral para mÃ¡xima privacidade

## ğŸ“‹ PrÃ©-requisitos

- **Python 3.8+**
- **Para execuÃ§Ã£o local**: Ollama + modelo Mistral (recomendado)
- **Para execuÃ§Ã£o com Docker**: Docker e Docker Compose

## ğŸ”§ InstalaÃ§Ãµes

### OpÃ§Ã£o 1: ExecuÃ§Ã£o Local (Recomendada)

1. **Instale o Ollama:**
   - Baixe de: <https://ollama.com/download>
   - Execute o instalador
   - Reinicie o terminal

2. **Baixe o modelo Mistral:**

   ```bash
   ollama pull mistral
   ```

3. **Clone e configure o projeto:**

   ```bash
   git clone [seu-repositorio]
   cd docIA
   python -m venv .venv
   
   # Windows
   .venv\Scripts\activate
   
   # Linux/Mac
   source .venv/bin/activate
   
   pip install -r requirements.txt
   ```

4. **Execute:**

   ```bash
   python smart_app.py
   ```

### OpÃ§Ã£o 2: ExecuÃ§Ã£o com Docker

1. **Execute com Docker Compose:**

   ```bash
   docker-compose up --build
   ```

   O Docker automaticamente:
   - Instala o Ollama
   - Baixa o modelo Mistral
   - Configura tudo para funcionar

## ğŸ“ Como Usar

1. **Adicione seus documentos** na pasta `documents/`
2. **Acesse**: <http://localhost:5000>
3. **FaÃ§a perguntas** sobre o conteÃºdo dos documentos
4. **Reindexe** quando adicionar novos arquivos (ou aguarde a detecÃ§Ã£o automÃ¡tica)

### Exemplos de Perguntas

- "Quais foram as principais decisÃµes da Ãºltima reuniÃ£o?"
- "HÃ¡ alguma menÃ§Ã£o sobre orÃ§amento?"
- "Quem foram os participantes da reuniÃ£o de marÃ§o?"
- "Resumo dos pontos discutidos sobre marketing"

## ğŸ§  Modelos de IA

O sistema prioriza sempre o **modelo Mistral via Ollama** para mÃ¡xima qualidade e privacidade:

1. **ğŸ¯ Mistral (PadrÃ£o)**: Modelo local de alta qualidade
2. **âš ï¸ Sistema Interno**: Fallback se Ollama nÃ£o estiver disponÃ­vel

### ConfiguraÃ§Ã£o do Modelo

O sistema estÃ¡ configurado para usar sempre o **Mistral** como padrÃ£o:

- **Localmente**: Detecta automaticamente se Mistral estÃ¡ disponÃ­vel
- **Docker**: Automaticamente baixa e configura o Mistral
- **VariÃ¡vel de ambiente**: `OLLAMA_MODEL=mistral` (jÃ¡ configurado)

## ğŸ“Š Status do Sistema

A interface mostra:

- âœ… **Ollama - mistral**: Funcionando perfeitamente
- âš ï¸ **Sistema Interno**: Funcional, mas qualidade limitada

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### VariÃ¡veis de Ambiente

```bash
OLLAMA_MODEL=mistral          # ForÃ§a uso do Mistral
FLASK_ENV=production          # Modo de produÃ§Ã£o
TRANSFORMERS_CACHE=/app/.cache # Cache dos modelos
```

### PersonalizaÃ§Ã£o

- **Tamanho dos chunks**: Modifique `chunk_size` em `smart_indexer.py`
- **NÃºmero de resultados**: Ajuste `max_results` nas buscas
- **Threshold de similaridade**: Configure em `_semantic_search`

## ğŸ³ Detalhes do Docker

O `Dockerfile.smart` inclui:

- InstalaÃ§Ã£o automÃ¡tica do Ollama
- Download do modelo Mistral
- ConfiguraÃ§Ã£o de cache otimizada
- InicializaÃ§Ã£o automÃ¡tica dos serviÃ§os

## ğŸ“ Estrutura do Projeto

```
docIA/
â”œâ”€â”€ smart_app.py           # AplicaÃ§Ã£o Flask principal
â”œâ”€â”€ smart_indexer.py       # Motor de busca e IA
â”œâ”€â”€ documents/             # Pasta dos documentos
â”œâ”€â”€ Dockerfile.smart       # Container com Ollama+Mistral
â”œâ”€â”€ docker-compose.yml     # OrquestraÃ§Ã£o completa
â”œâ”€â”€ requirements.txt       # DependÃªncias Python
â””â”€â”€ README.md             # Este arquivo
```

## ğŸš€ Performance

- **IndexaÃ§Ã£o**: ~100 documentos/minuto
- **Busca**: <1 segundo por consulta
- **MemÃ³ria**: ~200MB base + modelo IA
- **CPU**: Otimizado para uso eficiente

## ğŸ”’ Privacidade

- **100% Local**: Nenhum dado sai da sua mÃ¡quina
- **Sem APIs externas**: Modelo IA roda localmente
- **Controle total**: VocÃª possui todos os dados

## ğŸ› ï¸ Troubleshooting

### Problema: "Sistema Interno" em vez de Mistral

**SoluÃ§Ã£o:**

1. Verifique se Ollama estÃ¡ rodando: `ollama list`
2. Instale o Mistral: `ollama pull mistral`
3. Reinicie a aplicaÃ§Ã£o

### Problema: Baixa qualidade nas respostas

**SoluÃ§Ã£o:**

1. Instale o Ollama + Mistral (modelo mais poderoso)
2. Adicione mais documentos relevantes
3. Reformule a pergunta de forma mais especÃ­fica

### Problema: Documentos nÃ£o indexados

**SoluÃ§Ã£o:**

1. Verifique se estÃ£o na pasta `documents/`
2. Clique em "Reindexar" na interface
3. Verifique os logs no terminal

## ğŸ“Š VersÃ£o Atual

**v2.2.0** - Mistral Otimizado

- Prompt melhorado para respostas mais detalhadas
- ConfiguraÃ§Ãµes de temperatura e tokens otimizadas
- Dockerfile corrigido para garantir inicializaÃ§Ã£o do Ollama
- Logs simplificados sem emojis para compatibilidade
- ForÃ§a uso do Mistral como modelo obrigatÃ³rio

- âœ… Modelo Mistral configurado como padrÃ£o
- âœ… PriorizaÃ§Ã£o automÃ¡tica do Ollama+Mistral
- âœ… Fallback inteligente se IA nÃ£o estiver disponÃ­vel
- âœ… Interface atualizada com status do modelo
- âœ… Docker otimizado com Mistral prÃ©-configurado

---

**ğŸ’¡ Dica**: Para melhor experiÃªncia, use sempre o Ollama + Mistral. O sistema funciona sem ele, mas a qualidade das respostas Ã© significativamente superior com IA local!
